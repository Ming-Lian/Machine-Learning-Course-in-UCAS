<a name="content">目录</a>

[学习笔记：斯坦福大学李飞飞机器视觉公开课](#title)
- [CV历史回顾](#history)
- [k近邻与线性分类器](#k-nearest-and-linear-classification)
	- [k近邻](#k-nearest)
		- [超参数与超参数的选择](#hyper-parametric-and-choies)
	- [线性分类器](#linear-classification)
		- [损失函数](#loss-function)
		- [正则化](#regularization)
		- [Softmax Classifier](#softmax)
		- [Softmax vs. SVM](#softmax-vs-svm)
	- [最优化](#optimization)
		- [Mini-batch Gradient Descent](#mini-batch)
- [神经网络初步](#initial-neural-network)
	- [反向传播](#back-propagation)
		- [计算图与计算图导数](#computational-graph-and-derivative)
		- [实例](#bp-example)
		- [深度学习开源框架中的例子](#example-on-opensource-framework)
			- [Torch](#torch)
			- [Caffe](#caffe)
		- [向量化与Jacobian矩阵的稀疏性](#vectorize-and-sparsity-of-jacobian-matrix)
	- [神经网络](#neural-network)
		- [从线性分类器到神经网络](#compare-linear-classifier-and-nn)
		- [训练神经网络](#train-neural-network)
		- [激活函数](#activation-function)
		- [构建神经网络](#form-neural-network)
- [神经网络训练细节](#details-to-train-neural-network)
	- [训练中的错误认识：小训练集照样可行](#myth-toward-training)
	- [神经网络发展回顾](#a-bit-of-history)
	- [Activation Function](#detail-activation-function)
	- [数据预处理](#data-preprocessing)
	- [初始化权值](#weight-initialization)
	- [Batch Normalization](#batch-normalization)
	- [Babysitting the Learning Process](#babysitting)
		- [Double check the loss](#double-check-loss)
		- [完整性检查](#integrity-check)
		- [学习率的调整](#adjust-learning-rate)
	- [超参数优化](#hyperparameter-optimization)
		- [策略一：粗糙到精细](#hyperparameter-optimization-strategy-1)
		- [策略二：网格搜索](#hyperparameter-optimization-strategy-2)




<h1 name="title">学习笔记：斯坦福大学李飞飞机器视觉公开课</h1>

<a name="history"><h2>CV历史回顾 [<sup>目录</sup>](#content)</h2></a>

<p align="center"><img src=./picture/CV-Note1.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note3-1.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-ImageNet-error-change.png width=500 /></p>

<p align="center"><img src=./picture/CV-Note3-2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note4-1.jpg width=800 /></p>

<a name="k-nearest-and-linear-classification"><h2>k近邻与线性分类器 [<sup>目录</sup>](#content)</h2></a>

<p align="center"><img src=./picture/CV-Note4-2.jpg width=800 /></p>

<a name="k-nearest"><h3>k近邻 [<sup>目录</sup>](#content)</h3></a>

显示编程

```
def predict(image):
	# ???
	return class_lable
```

数据驱动

<p align="center"><img src=./picture/CV-data-driven-approach.png width=600 /></p>


<a name="hyper-parametric-and-choies"><h4>超参数与超参数的选择 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Note5.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-knn-example.png width=800 /></p>


<p align="center"><img src=./picture/CV-Note6-1.jpg width=800 /></p>

<table>
<th>5折验证</th>
<tr>
	<td><p align="center"><img src=./picture/CV-knn-5folds-validation.png width=800 /></p></td>
</tr>
<tr>
	<td><p align="center"><img src=./picture/CV-knn-5folds-cross-validation.png width=800 /></p></td>
</tr>
</table>

<p align="center"><img src=./picture/CV-knn-5folds-cross-validation-accuracy-curve.png width=500 /></p>

<p align="center"><img src=./picture/CV-Note6-2.jpg width=800 /></p>

<a name="linear-classification"><h3>线性分类器 [<sup>目录</sup>](#content)</h3></a>

**应用实例：用神经网络描述图片，CNN+RNN**

<p align="center"><img src=./picture/CV-application-example-cnn-plus-rnn.png width=800 /></p>

- 卷积神经网络：用于计算机视觉
- 循环神经网络：适用于排序问题

<p align="center"><img src=./picture/CV-Note7-1.jpg width=800 /></p>

<a name="loss-function"><h4>损失函数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-linear-classification-score-example.png width=500 /></p>

<p align="center"><img src=./picture/CV-linear-classification-loss-function.png width=400 /></p>

<p align="center"><img src=./picture/CV-linear-classification-loss-function-usage.png width=500 /></p>

总体损失函数：

<p align="center"><img src=./picture/CV-linear-classification-loss-function-total.png width=300 /></p>

思考：

<p align="center"><img src=./picture/CV-Note7-2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note8-1.jpg width=800 /></p>

python代码

<p align="center"><img src=./picture/CV-linear-classification-loss-function-code.png width=800 /></p>

但是这个损失函数其实是有问题的！！！

<p align="center"><img src=./picture/CV-linear-classification-loss-function-bug.png width=800 /></p>

当我们在一个数据集中找到一个使损失函数为0的W，那么这个W是**唯一**的吗？

**不唯一**，当找到一个满足使损失函数为0的W时，对其进行线性缩放，可以得到一系列的W空间，并且根据这个损失函数它们的工作方式都相同，它们都满足使损失函数为0

<a name="regularization"><h4>正则化 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Note9-1.jpg width=800 /></p>

L2 regularization的例子：

<p align="center"><img src=./picture/CV-linear-classification-L2-regularization-example.png width=400 /></p>

<p align="center"><img src=./picture/CV-Note9-2.jpg width=800 /></p>

<a name="softmax"><h4>Softmax Classifier [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Note10-1.jpg width=800 /></p>

计算损失函数示例：

<p align="center"><img src=./picture/CV-Softmax-classifier-loss-function-usage.png width=800 /></p>

<p align="center"><img src=./picture/CV-Note10-2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note11-1.jpg width=800 /></p>

<a name="softmax-vs-svm"><h4>Softmax vs. SVM [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Note11-2.jpg width=800 /></p>

<a name="optimization"><h3>最优化 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/CV-Note12-1.jpg width=800 /></p>

数值梯度方法代码：

<p align="center"><img src=./picture/CV-optimization-follow-slope-code.png width=800 /></p>

- approximate
- very slow to evaluate

运算速度慢，且求得的数值并不准确

解决方法：**牛顿-莱布尼茨微积分**

<p align="center"><img src=./picture/CV-Note12-2.jpg width=800 /></p>

梯度下降法代码：

<p align="center"><img src=./picture/CV-optimization-gradient-descent-code.png width=800 /></p>

<p align="center"><img src=./picture/CV-Note13-1.jpg width=800 /></p>

<a name="mini-batch"><h4>Mini-bacth Gradient Descent [<sup>目录</sup>](#content)</h4></a>

only use a small portion of the training set to compute the gradient

代码：

<p align="center" ><img src=./picture/CV-optimization-mini-batch-code.png width=800 /></p>

<p align="center"><img src=./picture/CV-Note13-2.jpg width=800 /></p>

<p align="center"><strong>Loss over mini-batch goes down over time</strong></p>

<p align="center"><img src=./picture/CV-optimization-mini-batch-loss-descent-trend.png width=500 /></p>

<p align="center"><img src=./picture/CV-optimization-effects-of-stepsize.png width=500 /></p>

**建议**：一开始选用一个比较大的学习率，然后随着迭代次数的增加，逐渐将学习率调小

<a name="initial-neural-network"><h2>神经网络初步 [<sup>目录</sup>](#content)</h2></a>

<a name="back-propagation"><h3>反向传播 [<sup>目录</sup>](#content)</h3></a>

<a name="computational-graph-and-derivative"><h4>计算图与计算图导数 [<sup>目录</sup>](#content)</h4></a>

该部分请参考[《学习笔记：神经网络与深度学习》](https://github.com/Ming-Lian/Machine-Learning-Course-in-UCAS/blob/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.md#computation-graph-deviation)

<a name="bp-example"><h4>实例 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-bp-example-function-formula.png width=400 /></p>

要用到的导数公式：

<p align="center"><img src=./picture/CV-bp-derivative-formulas.png width=800 /></p>

Back propagation过程：

<p align="center"><img src=./picture/CV-bp-process-example-1.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-2.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-3.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-4.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-5.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-6.png width=600 /></p>

计算图中不同计算单元在梯度流动中的作用：
- **add gate**: gradient distributor——每个输入分配相同的梯度
- **max gate**: gradient router——较大值的局部梯度为1，较小值局部梯度为0
- **multi gate**: gradient… “switcher”?

<p align="center"><img src=./picture/CV-bp-backward-flow-pattern.png width=600 /></p>

前向/反向传播伪代码：

<p align="center"><img src=./picture/CV-bp-forward-and-backward-psudo-code.png width=800 /></p>

<a name="example-on-opensource-framework"><h4>深度学习开源框架中的例子 [<sup>目录</sup>](#content)</h4></a>

<a name="torch"><h4>Torch [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Torch-forward.png width=450 /></p>

<p align="center"><img src=./picture/CV-Torch-backward.png width=500 /></p>

<a name="caffe"><h4>Caffe [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-caffe-forward.jpg width=600 /></p>

<p align="center"><img src=./picture/CV-caffe-backward.jpg width=600 /></p>

<a name="vectorize-and-sparsity-of-jacobian-matrix"><h4>向量化与Jacobian矩阵的稀疏性 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-bp-vectorize.png width=600 /></p>

雅克比矩阵（一阶偏导数以一定方式排列成的矩阵）可能存在一定的稀疏性，则可以不需要将整个矩阵求出来，从而提高算法效率

雅克比矩阵稀疏性的例子：

<p align="center"><img src=./picture/CV-bp-vectorize-Jacobian-matrix-sparsity.jpg width=600 /></p>

反向求梯度获得Jacobian matrix，其元素值要么是1，要么是0，则只需要观察那些小于0的输入，然后将这些维度上的梯度置为0

<a name="neural-network"><h3>神经网络 [<sup>目录</sup>](#content)</h3></a>

<a name="compare-linear-classifier-and-nn"><h4>从线性分类器到神经网络 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-compare-linear-classifier-and-nn-1.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-compare-linear-classifier-and-nn-2.jpg width=800 /></p>

<a name="train-neural-network"><h4>训练神经网络 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-neural-network-implementation-of-training-2-layer-nn.png width=800 /></p>

<p align="center"><img src=./picture/CV-neural-network-implementation-of-training-2-layer-nn-code.png width=600 /></p>

<a name="activation-function"><h4>激活函数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-neural-network-activation-function-1.png width=500 /></p>

<table>
<tr>
	<td><img src=./picture/CV-neural-network-activation-function-2.png width=350 /></td>
	<td><img src=./picture/CV-neural-network-activation-function-3.png width=500 /></td>
</tr>
</table>

历史上，一开始使用的是tanh函数，2012年 **ReLU** 函数开始流行（让神经网络快很多，**推荐作为默认选择**），

<a name="form-neural-network"><h4>构建神经网络 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-neural-network-architecture.png width=800 /></p>

增加隐含层神经元数，带来非线性分类性能的提升

<p align="center"><img src=./picture/CV-neural-network-more-neurons-more-capacity.png width=800 /></p>

加入正则化（对高维度W进行惩罚，防止过拟合）

<p align="center"><img src=./picture/CV-neural-network-affect-of-regularizer.png width=800 /></p>

<a name="details-to-train-neural-network"><h2>神经网络训练细节 [<sup>目录</sup>](#content)</h2></a>

<a name="myth-toward-training"><h3>训练中的错误认识：小训练集照样可行 [<sup>目录</sup>](#content)</h3></a>

迁移应用过程：

<table>
<tr>
	<td><img src=./picture/CV-transfer-learning-with-cnn-1.png width=250 /></td>
	<td><img src=./picture/CV-transfer-learning-with-cnn-2.png width=350 /></td>
	<td><img src=./picture/CV-transfer-learning-with-cnn-3.png width=400 /></td>
</tr>
</table>

在ImageNet的预训练已经有人做好了，并且已经将自己训练好的模型（CNNs的权值数据）上传至 [Caffe model zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)

<a name="a-bit-of-history"><h3>神经网络发展回顾 [<sup>目录</sup>](#content)</h3></a>

(1) 1957年：Frank Rosenblatt —— Perceptron，**感知器**

<p align="center"><img src=./picture/CV-history-of-neural-network-perceptron.png width=400 /></p>

<p align="center"><img src=./picture/CV-history-of-neural-network-perceptron-activation-function.png width=200 /></p>

其激活函数使用的是二进制阶梯函数，没有微分运算，也就不能进行反向传播

没有提出损失函数的概念，也没有反向传播的概念，只有以下特定的学习法则：

<p align="center"><img src=./picture/CV-history-of-neural-network-perceptron-update-rule.png width=300 /></p>

(2) 1960年：Widrow and Hoff —— Adaline/Madaline，**适应性神经元**

将感知器整合成一个多层次的感知器网络

(3) 1986年，突破性进展：Rumelhart et al —— **第一次用清晰的公式很好地阐述了反向传播的概念**

找到了一个让人信服的原则，通过反向传播可以对神经网络进行训练

不幸的是，当时他们将神经网络设计得更深时，与其他机器学习工具包相比，它们的工作效果并不好

**随后该领域沉寂了近20年**

(4) 2006年，该领域研究复兴：**Hinton** and Salakhutdinov —— nature上发表论文，第一次建立神经网络模型

训练步骤：
- 首先是逐层进行预训练，使用RBM（受限玻尔兹曼机）
	
	不是在单通道中对所有的层都经过反向传播，相对于在训练第一层时使用一个无监督的目标，然后在此基础上训练第二层，接着训练第三层，第四层...
	
- 第一步训练完成后把它们整合起来，然后进行反向传播和细调

<p align="center"><img src=./picture/CV-history-of-neural-network-hinton.png width=800 /></p>

此时，人们已经能够对神经网络进行训练了，但依然没有很多人注意到这个领域，直到2010年至2012年

(5) 2010-2012年，深度学习兴起

- 2010年

	将神经网络用在GMM-HMM语音识别模型中，将其中一部分改变成神经网络的一些东西，很好地提高了识别效果
	
<p align="center"><img src=./picture/CV-history-of-neural-network-2010.png width=500 /></p>

- 2012年

	机器视觉的图像识别领域：由Alex Krizhersky等人实现的网络在ImageNet挑战赛中碾压其他对手

<table align="center">
<tr>
	<td><img src=./picture/CV-history-of-neural-network-2012-1.png width=600 /></td>
</tr>
<tr>
	<td><img src=./picture/CV-history-of-neural-network-2012-2.png width=600 /></td>
</tr>
</table>

<a name="detail-activation-function"><h3>Activation Function [<sup>目录</sup>](#content)</h3></a>

**1\. sigmoid** 

- 会导致在反向传播算法中出现梯度趋于0的问题 —— 梯度弥散（消失）

<p align="center"><img src=./picture/CV-training-activation-function-sigmoid-gradient-loss.png width=600 /></p>

<p align="center"><img src=./picture/CV-Note15.jpg width=600 /></p>


- sigmoid函数的输出不是关于原点中心对称的，输出在(0,1)范围内

	Q：当网络中每个神经元的输出都为正值，且为(0,1)时，会发生什么？

<p align="center"><img src=./picture/CV-neural-network-implementation-of-training-2-layer-nn.png width=800 /></p>

由于网络中每个神经元的输出都为正值，且为 (0,1) ，即 0< l<sub>i</sub> <1

则可推出 dl<sub>i</sub> / dl<sub>i-1</sub> = l<sub>i-1</sub> * (1 - l<sub>i-1</sub>) > 0

<p align="center"><img src=./picture/CV-derivation-of-dw-from-sigmoid.jpg width=300 /></p>

则反向传播的初始梯度**dL / dl<sub>n</sub>** 的正负取值会直接全盘影响整个网络中的dw<sub>i</sub>的正负取值，**要么全为正，要么全为负**

- 和其他函数相比，该函数中exp( )的计算很耗时

**2\. tanh** —— Yan LeCun 在1991年提出

关于原点中心对称

梯度依然可能饱和

**3\. ReLU —— max ( 0 , x )** ，Krizhevsky 在2012年提出

相比sigmoid，收敛速度快了6倍

原因：
> - Does not saturate (in +region)
> - Very computationally efficient —— 只是进行比较操作而已
> - Converges much faster than sigmoid/tanh in practice (e.g. 6x)

缺点：不是关于原点中心对称

<p align="center"><img src=./picture/CV-Note16-1.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-training-activation-function-ReLU-dead-ReLU.png width=800 /></p>

<p align="center"><img src=./picture/CV-Note16-2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note17.jpg width=800 /></p>

**4\. Leaky ReLU $nbsp;$nbsp;$nbsp; f(x) = max( 0.01x, x )**

<p align="center"><img src=./picture/CV-Note18-1.jpg width=800 /></p>

**5\. Exponential Linear Units (ELU)** 2015

<p align="center"><img src=./picture/CV-training-activation-function-ELU.png width=500 /></p>

<p align="center"><img src=./picture/CV-training-activation-function-ELU-advantage-disadvantage.png width=400 /></p>

**6\. Maxout "Neuron"** 2013年，由Goodfellow提出

<p align="center"><img src=./picture/CV-Note18-2.jpg width=800 /></p>

<a name="data-preprocessing"><h3>数据预处理 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/CV-training-data-preprocessing-1.png width=800 /></p>

归一化方法在图像处理中并不常用，但0中心化应用很多

<p align="center"><img src=./picture/CV-training-data-preprocessing-2.png width=800 /></p>

<p align="center"><img src=./picture/CV-Note19-1.jpg width=800 /></p>

它们在图像处理中并没有应用

在图像处理中用到的数据预处理方法只有**中心化**，例如：

<p align="center"><img src=./picture/CV-training-data-preprocessing-center-only-example.png width=800 /></p>

<a name="weight-initialization"><h3>初始化权值 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/CV-Note19-2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-training-weight-initialization-ipython-notebook-1.png width=800 /></p>

<p align="center"><img src=./picture/CV-training-weight-initialization-ipython-notebook-2.png width=800 /></p>

<p align="center"><img src=./picture/CV-training-weight-initialization-ipython-notebook-3.png width=800 /></p>

- 先看逐层的输出的均值与方差的变化趋势

<p align="center"><img src=./picture/CV-training-weight-initialization-ipython-notebook-output.png width=500 /></p>

<table>
<tr>
	<td><img src=./picture/CV-training-weight-initialization-ipython-notebook-output-mean.png width=500 /></td>
	<td><img src=./picture/CV-training-weight-initialization-ipython-notebook-output-std.png width=500 /></td>
</tr>
</table>

由于激活函数选择的是tanh，所以神经元输出的均值会归于0左右

其方差一开始是1，接下来的层中逐渐下降到0

- 再分别统计每层的输入（或输出）的分布情况

<p align="center"><img src=./picture/CV-training-weight-initialization-ipython-notebook-output-histogram.png width=900 /></p>

第一张图是合理的，数据处于-1到1之间

然后这些数据分布开始“坍缩”，最终只分布在0上，即在这个网络中，最终所有的以tanh为激活函数的神经元，输出都是0


<p align="center"><img src=./picture/CV-training-weight-initialization-xavier-output-histogram.png width=900 /></p>

基本规则：一些大的标准差，基本上是靠谱的初始化方法

从图中可以看出，


比较有无因子2两种情况下，损失函数的收敛情况：

<p align="center"><img src=./picture/CV-training-weight-initialization-xavier-compare-loss-gradiant.png width=600 /></p>


<a name="batch-normalization"><h3>Batch Normalization [<sup>目录</sup>](#content)</h3></a>

该部分可以参考 [《学习笔记：改善深层神经网络——Batch正则化》](https://github.com/Ming-Lian/Machine-Learning-Course-in-UCAS/blob/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.md#batch-normalization)

基本主旨：让神经网络的每一层、一批数据的每个特征维度，都有粗略的单位高斯激活（unit gaussian activation）

<p align="center"><img src=./picture/CV-training-batch-normalization-process.png width=800 /></p>

保证每一列（代表一个维度的特征）的每个单位都是 unit gaussian

执行Batch Normalization的位置：

<p align="center"><img src=./picture/CV-training-batch-normalization-insert-location.png width=800 /></p>

Q: unit guassian的输入对于非线性激活函数是否总是合适？

测试时BN操作稍有不同，其γ和β的获得：
- 基于整个训练集一次性算出来；
- 在训练过程中，基于各个mini-batich的均值和方差，进行指数加权平均，如下图：

<p align="center"><img src=./picture/Improving-DeepNeuralNetwork-week3-7-2.jpg width=800 /></p>

<a name="babysitting"><h3>Babysitting the Learning Process [<sup>目录</sup>](#content)</h3></a>

<a name="double-check-loss"><h4>Double check the loss [<sup>目录</sup>](#content)</h4></a>

例子：两层神经网络，进行10类别的判别，最后使用的是softmax分类器

**先初始化权值**：由于使用的是两层的浅层网络，采用原始的权值初始化方法即可

<p align="center"><img src=./picture/CV-training-babysitting-weight-initialization-code.png width=800 /></p>

然后进行**前向传播**，得到损失（loss）

- **1st check**： 取消正则化，来检查损失值是否正确

期望的损失值应该为-log(1/10)，想知道原因请点 [这里](#softmax)

<p align="center"><img src=./picture/CV-training-babysitting-1st-check-loss.png width=800 /></p>

- **2nd check**： 启动正则化，预期损失值会上升

因为在原始函数里多加了一个新的项 —— 正则化项

<p align="center"><img src=./picture/CV-training-babysitting-2nd-check-loss.png width=800 /></p>

<a name="integrity-check"><h4>完整性检查 [<sup>目录</sup>](#content)</h4></a>

对于一个神经网络，尝试使用训练数据的一小部分进行训练，测试神经网络是否可以得到一个过拟合（overfit）的模型 —— **网络的训练可以正常进行，即反向传播应该在正常工作，学习率的设置也比较合理**

<p align="center"><img src=./picture/CV-training-babysitting-integrity-check-code.png width=800 /></p>

取20个训练样本和20个标签，然后保证模型基于这一小部分数据训练，最终可以得到损失值基本为0，即实现完全过拟合

<p align="center"><img src=./picture/CV-training-babysitting-integrity-check-output.png width=800 /></p>

<a name="adjust-learning-rate"><h4>学习率的调整 [<sup>目录</sup>](#content)</h4></a>

学习率太低的情况：

<p align="center"><img src=./picture/CV-training-babysitting-adjust-learning-rate-too-low.png width=800 /></p>

学习率太高的情况：

<p align="center"><img src=./picture/CV-training-babysitting-adjust-learning-rate-too-high.png width=800 /></p>

<a name="hyperparameter-optimization"><h3>超参数优化 [<sup>目录</sup>](#content)</h3></a>

粗糙到精细化的思想：
> - First stage: only a few epochs to get rough idea of what params work
> - Second stage: longer running time, finer search
>     … (repeat as necessary)
> - 最后选出一个表现最好的参数

<p align="center"><strong>coarse search</strong></p>

<p align="center"><img src=./picture/CV-training-hyperparameter-optimization-coarse.png width=800 /></p>

<p align="center"><strong>finer search</strong></p>

<p align="center"><img src=./picture/CV-training-hyperparameter-optimization-finer.png width=800 /></p>

<p align="center"><img src=./picture/CV-training-hyperparameter-optimization-finer-output.png width=400 /></p>

<a name="hyperparameter-optimization-strategy-2"><h4>策略二：网格搜索 [<sup>目录</sup>](#content)</h4></a>

使用一个固定的步长，在学习率和正则化系数上每一步取一个值

但是，**其性能不如随机取值好**

<p align="center"><img src=./picture/CV-training-hyperparameter-optimization-compare-2-strategy.png width=600 /></p>

前提条件：在超参数优化过程中，经常发生的是，**一个超参数的重要性会远远高于另一个**



