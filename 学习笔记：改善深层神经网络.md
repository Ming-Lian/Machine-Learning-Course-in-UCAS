<a name="content">目录</a>

[改善深层神经网络](#title)
- [第一周：深度学习的实用层面](#week1)
	- [1.1. 深度学习的训练](#training-of-deeplearning)
	- [1.2. 偏差与方差](#bias-and-variance)
		- [1.2.1. 调整偏差与方差](#adjustment-of-bias-and-variance)
	- [1.3. 正则化](#regularization)
		- [1.3.1. 正则化防止过拟合的原因](#reason-of-regularization-preventing-overfit)
		- [1.3.2. dropout正则化](#dropout-regularization)
		- [1.3.3. 理解dropout](#understand-dropout)
		- [1.3.4. 其他正则化方法](#another-way-to-regularization)
			- [1.3.4.1. 扩增训练数据](#agument-traning-set)
			- [1.3.4.2. Early stopping](#early-stopping)
	- [1.4. 加速训练](#speedup-traning)
		- [1.4.1. 正则化输入](#regularize-input)
		- [1.4.2. 梯度消失/爆炸](#gradient-vanish-or-explosion)
			- [1.4.2.1. 解决方法：权重矩阵初始化](#weight-initialization)
	- [1.5. 梯度检查](#grad-check)
		- [1.5.1. 梯度逼近](#gradient-approximation)
		- [1.5.2. 实施过程](#operating-program)
- [第二周：优化算法](#week2)
	- [2.1. Mini-batch梯度下降方法](#mini-batch-gradient-descent)
		- [2.1.1. 什么是Mini-batch](#what-is-mini-batch)
		- [2.1.2. 理解Mini-batch](#understand-mini-batch)
	- [2.2. 指数加权平均](#exponentially-weighted-averages)
		- [2.2.1. 说明是指数加权平均](#what-is-exponentially-weighted-averages)
		- [2.2.2. 理解指数加权平均](#understand-exponentially-weighted-averages)
		- [2.2.3. 实施指数加权平均及其偏差修正](#carryout-bias-correction)
	- [2.3. Momentum梯度下降法](#gradient-descent-with-momentum)
	- [2.4. RMSprp梯度下降法](#gradient-descent-with-emsprop)
	- [2.5. Adam优化算法: Momentum + RMSprp](#adam)
	- [2.6. 学习率衰减](#learning-rate-decay)
	- [2.7. 解决局部最优问题](#local-optima)
- [第三周：超参数调试、Batch正则化和程序框架](#week3)
	- [3.1. 超参数调试](#debug-hyperparameters)
		- [3.1.1. 搜索超参数空间](#search-hyperparameters-space)
		- [3.1.2. 为超参数选择合适的搜索尺牍（范围）](#chose-proper-scale)
		- [3.1.3. 超参数训练的两大策略](#hyperparameters-training-strategies)
	- [3.2. Batch正则化](#batch-normalization)
		- [3.2.1. 正则化网络的激活函数](#normalizing-activation-in-a-network)
		- [3.2.2. 实施Batch归一化](#implementing-batch-norm)


<h1 name="title">改善深层神经网络</h1>

<p align="center"><img src=./picture/deeplearning.ai.logo.png width="400" /></p>

<a name="week1"><h3>第一周：深度学习的实用层面 [<sup>目录</sup>](#content)</h3></a>

<a name="training-of-deeplearning"><h4>1.1. 深度学习的训练 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week1-2-1.jpg width="800" />

<a name="bias-and-variance"><h4>1.2. 偏差与方差 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-bais-and-variance.png width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week1-2-2.jpg width="800" />

高偏差与高方差的例子：

<img src=./picture/Improving-DeepNeuralNetwork-2hign-example.png width="800" />

用紫色线画出的分类器具有高偏差和高方差，**高偏差**：它几乎是一条线性分类器，并未拟合数据；**高方差**：曲线中间部分灵活性非常高，却过度拟合了两个错误样本

<a name="adjustment-of-bias-and-variance"><h4>1.2.1. 调整偏差与方差 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-3.jpg width="800" />

<a name="regularization"><h4>1.3. 正则化 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-4.jpg width="800" />

<a name="reason-of-regularization-preventing-overfit"><h4>1.3.1. 正则化防止过拟合的原因 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-5-1.jpg width="800" />

<a name="dropout-regularization"><h4>1.3.2. dropout正则化 [<sup>目录</sup>](#content)</h4></a>

<table>
<tr>
	<td><img src=./picture/Improving-DeepNeuralNetwork-dropout-regularization-1.png width="400" /></td>
	<td><img src=./picture/Improving-DeepNeuralNetwork-dropout-regularization-2.png width="400" /></td>
</tr>
</table>

<img src=./picture/Improving-DeepNeuralNetwork-week1-5-2.jpg width="800" />

<a name="understand-dropout"><h4>1.3.3. 理解dropout [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-6-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week1-6-2.jpg width="800" />

<a name="another-way-to-regularization"><h4>1.3.4. 其他正则化方法 [<sup>目录</sup>](#content)</h4></a>

<a name="agument-traning-set"><h4>1.3.4.1. 扩增训练数据 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-7-1.jpg width="800" />

<a name="early-stopping"><h4>1.3.4.2. Early stopping [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-7-2.jpg width="800" />

<a name="speedup-traning"><h4>1.4. 加速训练  [<sup>目录</sup>](#content)</h4></a>

<a name="regularize-input"><h4>1.4.1. 加速训练方法一：正则化输入 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-7-3.jpg width="800" />

<table>
<tr>
	<td><img src=./picture/Improving-DeepNeuralNetwork-week2-unnormalized-1.png width="400" /></td>
	<td><img src=./picture/Improving-DeepNeuralNetwork-week2-normalized-1.png width="400" /></td>
</tr>
<tr>
	<td><img src=./picture/Improving-DeepNeuralNetwork-week2-unnormalized-2.png width="400" /></td>
	<td><img src=./picture/Improving-DeepNeuralNetwork-week2-normalized-2.png width="400" /></td>
</tr>
</table>

<img src=./picture/Improving-DeepNeuralNetwork-week1-8-1.jpg width="800" />

<a name="gradient-vanish-or-explosion"><h4>1.4.2. 梯度消失/爆炸 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-8-2.jpg width="800" />

<a name="weight-initialization"><h4>1.4.2.1. 解决方法：权重矩阵初始化 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-9-1.jpg width="800" />

<a name="grad-check"><h4>1.5. 梯度检查 [<sup>目录</sup>](#content)</h4></a>

<a name="gradient-approximation"><h4>1.5.1. 梯度逼近 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-9-2.jpg width="800" />

<a name="operating-program"><h4>1.5.2. 实施过程 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-10-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week1-10-2.jpg width="800" />

<a name="week2"><h3>第二周：优化算法 [<sup>目录</sup>](#content)</h3></a>

<a name="mini-batch-gradient-descent"><h4>2.1. Mini-batch梯度下降方法 [<sup>目录</sup>](#content)</h4></a>

<a name="what-is-mini-batch"><h4>2.1.1. 什么是Mini-batch [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-1.jpg width="800" />

<a name="understand-mini-batch"><h4>2.1.2. 理解Mini-batch [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-2-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week2-2-2.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week2-3-1.jpg width="800" />

<a name="exponentially-weighted-averages"><h4>2.2. 指数加权平均 [<sup>目录</sup>](#content)</h4></a>

<a name="what-is-exponentially-weighted-averages"><h4>2.2.1. 什么是指数加权平均 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-3-2.jpg width="800" />

<a name="understand-exponentially-weighted-averages"><h4>2.2.2. 理解指数加权平均 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-4-1.jpg width="800" />

<a name="carryout-bias-correction"><h4>2.2.3. 实施指数加权平均及其偏差修正 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-4-2.jpg width="500" />

<img src=./picture/Improving-DeepNeuralNetwork-week2-5-1.jpg width="800" />

<a name="gradient-descent-with-momentum"><h4>2.3. Momentum梯度下降法 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-5-2.jpg width="800" />

<a name="gradient-descent-with-emsprop"><h4>2.4. RMSprp梯度下降法 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-6.jpg width="800" />

<a name="adam"><h4>2.5. Adam优化算法: Momentum + RMSprp [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-7-1.jpg width="800" />

<a name="learning-rate-decay"><h4>2.6. 学习率衰减 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-7-2.jpg width="800" />

<a name="local-optima"><h4>2.7. 解决局部最优问题 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-local-optima-problem.png width="800" />

当特征空间为低维时，的确容易出现多个不同的局部最优，但是在高维度的空间中，如果梯度为0，更有可能碰到鞍点，而不是碰到局部最优。

<img src=./picture/Improving-DeepNeuralNetwork-week2-8-1.jpg width="800" />

<a name="week3"><h3>第三周：超参数调试、Batch正则化和程序框架 [<sup>目录</sup>](#content)</h3></a>

<a name="debug-hyperparameters"><h4>3.1. 超参数调试 [<sup>目录</sup>](#content)</h4></a>

<a name="search-hyperparameters-space"><h4>3.1.1. 搜索超参数空间 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-1.jpg width="800" />

<a name="chose-proper-scale"><h4>3.1.2. 为超参数选择合适的搜索尺牍（范围） [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-2.jpg width="800" />

<a name="hyperparameters-training-strategies"><h4>3.1.3. 超参数训练的两大策略 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-hyperparameters-training-strategies.png width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week3-3-1.jpg width="800" />

<a name="batch-normalization"><h4>3.2. Batch正则化 [<sup>目录</sup>](#content)</h4></a>

<a name="normalizing-activation-in-a-network"><h4>3.2.1. 正则化网络的激活函数 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-3-2.jpg width="800" />

<a name="implementing-batch-norm"><h4>3.2.2. 实施Batch归一化 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-4.jpg width="800" />
