<a name="content">目录</a>

[斯坦福大学机器学习公开课笔记](#title)
- [1. 符号约定](#sign)
- [2. 线性回归与优化方法](#linner-regression)
	- [2.1. 目标函数](#object-function)
	- [2.2. 优化方法一：批量梯度下降](#batch-gradient-descent)
	- [2.3. 优化方法二：随机梯度下降](#stochastic-gradient-descent)
	- [2.4. 优化方法三：线性代数方法](#linear-algebra)
	- [2.5. 参数与非参数学习算法](#parametric-learning-algorithm)
		- [2.5.1. 非参数学习算法的例子：Local weighted regression （局部加权回归）](#local-weighted-regression)
	- [2.6. 线性回归目标函数（最小二乘）的来源](#origin-of-object-function-linner-regression)
- [3. 分类](#classification)
	- [3.1. 对Logistic回归的理解](#understand-logistic-regression)
	- [3.2. 问题描述与优化方法一：梯度上升](#description)
	- [3.3. 优化方法二：Newtown's Method](#newtown-method)
	- [3.4. 目标函数 Logistic function 的来源](#origin-of-object-function-logistic)
		- [3.4.1. 指数分布族 (Exponential Family)](#exponential-family)
		- [3.4.2. 广义线性模型 (GLM)](#general-linner-model)
- [4. 生成学习算法](#generative-learning-algorithms)
	- [4.1. 比较：判别学习算法与生产学习算法](#compare)
	- [4.2. 多元高斯分布](#multi-gaussian-distribution)
	- [4.3. 生成学习算法一：高斯判别分析](#gaussian-discriminant-analysis)
		- [4.3.1. 高斯判别分析与logistic回归的关系](#relation-between-gda-logistic)
	- [4.4. 生成学习算法二：Naive Bayes](#naive-bayes)
		- [4.4.1. Laplace smoothing](#laplace-smoothing)
		- [4.4.2. Naive Bayes的一般化](#generalization-for-naive-bayes)
			- [4.4.2.1. x<sup>(i)</sup>有多个取值](#more-value-for-x)
			- [4.4.2.2. 多项式事件模型：考虑词出现的次数](#multinomial-event-model)
- [5. 非线性分类器](#nolinear-classification)
	- [5.1. Neural Network](#neural-network)
	- [5.2. SVM](#svm)
		- [5.2.1. 最大间隔分类器](#maximum-margin-classification)
			- [5.2.1.1. 函数间隔与几何间隔](#functional-and-geometric-margin)
				- [5.2.1.1.1. 函数间隔](#functional-margin)
				- [5.2.1.1.2. 几何间隔](#geometric-margin)
			- [5.2.1.2. 最大化间隔分类器优化目标](#maximum-margin-optimization-objective)
				- [5.2.1.2.1. 改变优化目标的表述方式](#change-optimization-objective)
				- [5.2.1.2.2. 拉格朗日乘数法](#lagrange-multipliers-approach)
			- [5.2.1.3. 对偶问题](#dual-problem)
				- [5.2.1.3.1 原始问题与对偶问题获得相同解的情况](#condtion-primal-problem-equal-to-dual-problem)
		- [5.2.2. Support Vector Machine](#support-vector-machine)
			- [5.2.2.1. 对SVM的理解](#understand-svm)
			- [5.2.2.2. 符号改动与优化目标](#svm-note-change-and-optimization-objective)
			- [5.2.2.3. 对偶问题及求解](#svm-dual-problem-and-solving)
			- [5.2.2.4. 算法內积化，引出Kernel](#svm-inner-product)
		- [5.2.3. 核函数Kernels](#kernels)
			- [5.2.3.1. 核函数定义](#define-kernels)
			- [5.2.3.2. Kernel如何实现高效计算？](#kernel-how-to-be-effective)
			- [5.2.3.3. 测试Kernel是否合法](#validate-kernel)
			- [5.2.3.4. Kernel在SVM中的应用](#use-kernel-in-svm)
		- [5.2.4. 解决非线性决策问题：L<sub>1</sub> Norm Soft Margin SVM](#svm-deal-with-unlinear-classification)
			- [5.2.4.1. 两种非线性决策情况](#two-type-unlinear-classification)
			- [5.2.4.2. L<sub>1</sub> Norm Soft Margin SVM](#soft-margin-svm)
		- [5.2.5. 坐标上升法](#digression-coordinate-ascent)
			- [5.2.5.1. 原算法](#original-coordinate-ascent)
			- [5.2.5.2. 算法改进：SMO算法](#smo-algorithm)
- [6. 学习理论](#learning-theory)
	- [6.1. 偏差-方差权衡](#bias-variance-trade-off)
		- [6.1.1. 偏差与方差的直观理解](#understand-bias-and-variance)
		- [6.1.2. 欠拟合与过拟合](#underfit-and-overfit)
		- [6.1.3. ERM（经验风险最小化）](#empirical-risk-minimization)
		- [6.1.4. 证明：|ε(h)-ε^(h)| 有上界](#prove-erm-bound)
			- [6.1.4.1. 概率界形式](#probability-bound)
			- [6.1.4.2. 样本复杂度界形式](#sample-complexity-bound)
			- [6.1.4.3. 误差界形式](#error-bound)
				- [6.1.4.3.1. 有助于量化偏差-方差权衡](#error-bound-benefit-for-quantify-bias-variance-trade-off)
		- [6.1.5. 探究样本复杂度](#investigate-sample-compexity)
			- [6.1.5.1. 基于浮点数 64bit 的不严谨推论](#investigate-sample-compexity-on-64bit)
			- [6.1.5.2. 正式表示形式：基于VC维](#investigate-sample-compexity-base-on-vc-dimension)
				- [6.1.5.2.1. VC维定义](#definition-of-vc-dimension)
				- [6.1.5.2.2. 基于VC维评估样本复杂度](#estimate-sample-compexity-base-on-vc-dimension)








<h1 name="title">斯坦福大学机器学习公开课笔记</h1>

<a name="sign"><h2>1. 符号约定 [<sup>目录</sup>](#content)</h2></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note1.jpg width="800" />

<a name="linner-regression"><h2>2. 线性回归与优化方法 [<sup>目录</sup>](#content)</h2></a>

<a name="object-function"><h3>2.1. 目标函数 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note2-1.jpg width="800" />

<a name="batch-gradient-descent"><h3>2.2. 优化方法一：批量梯度下降 (Batch Gradient Descent) [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note2-2.jpg width="800" />

<a name="stochastic-gradient-descent"><h3>2.3. 优化方法二：随机梯度下降 (Stochastic Gradient Descent) [<sup>目录</sup>](#content)</h3></a>

随机梯度下降方法与批量梯度下降相比，每次迈出一步不一定是下降最快的方向，有时甚至是相反的方向，但是它的总体趋势是下降的。迭代次数多，但是每次迭代所用的时间少，总体效率更高。

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note3-1.jpg width="800" />

<a name="linear-algebra"><h3>2.4. 优化方法三：线性代数方法 [<sup>目录</sup>](#content)</h3></a>

定义矩阵导数，以及矩阵的迹的性质

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note3-2.jpg width="800" />

用线性代数方法求解θ

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note4.jpg width="800" />

最终得到：

<p align="center"> θ = （X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>Y </p>

<a name="parametric-learning-algorithm"><h3>2.5. 参数与非参数学习算法 [<sup>目录</sup>](#content)</h3></a>

- Parametric Learning Algorithm : find set of parametric

- No-parametric Learning Algorithm : no of parametric goes with m

<a name="local-weighted-regression"><h4>2.5.1. 非参数学习算法的例子：Local weighted regression （局部加权回归） [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note5-1.jpg width="800" />

<a name="origin-of-object-function-linner-regression"><h3>2.6. 线性回归目标函数（最小二乘）的来源 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note6.jpg width="800" />

So maximize L(θ) is the same as minimize

<p align="center"><img src=/picture/Marchine-Learning-Stanford-AndrewNg-equation.png /></p>

<a name="classification"><h2>3. 分类 [<sup>目录</sup>](#content)</h2></a>

<a name="understand-logistic-regression"><h3>3.1. 对Logistic回归的理解 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-understand-logistic-regression-1.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-understand-logistic-regression-2.jpg width="800" />

<a name="description"><h3>3.2. 分类问题描述，及用梯度上升方法进行求解 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note7-1.jpg width="800" />

L(θ)的导数推导过程：

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-derivative.PNG width="800" />

<a name="newtown-method"><h3>3.3. 优化方法二：Newtown's Method [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note8.jpg width="800" />

<a name="origin-of-object-function-logistic"><h3>3.4. 目标函数 Logistic function 的来源 [<sup>目录</sup>](#content)</h3></a>

<a name="exponential-family"><h4>3.4.1. 指数分布族 (Exponential Family) [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note9.jpg width="800" />

<a name="general-linner-model"><h4>3.4.2. 广义线性模型 (GLM) [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note10.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note11.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note12-1.jpg width="800" />

<a name="generative-learning-algorithms"><h2>4. 生成学习算法 [<sup>目录</sup>](#content)</h2></a>

<a name="compare"><h3>4.1. 比较：判别学习算法与生产学习算法 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note13-1.jpg width="800" />

<a name="multi-gaussian-distribution"><h3>4.2. 多元高斯分布 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note13-2.jpg width="800" />

改变协方差矩阵主对角线上的数值

<table>
<tr>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian1.png width="300" /></td>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian2.png width="300" /></td>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian3.png width="300" /></td>
</tr>
</table>

改变协方差矩阵副对角线上的数值

<table>
<tr>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian4.png width="300" /></td>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian5.png width="300" /></td>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian6.png width="300" /></td>
</tr>
</table>

以等高线图展示

<table>
<tr>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian7.png width="800" /></td>
</tr>
<tr>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian8.png width="800" /></td>
</tr>
</table>

<a name="gaussian-discriminant-analysis"><h3>4.3. 生成学习算法一：高斯判别分析 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note14.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note15-1.jpg width="800" />

<a name="relation-between-gda-logistic"><h4>4.3.1. 高斯判别分析与logistic回归的关系 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note15-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note16.jpg width="800" />

<a name="naive-bayes"><h3>4.4. 生成学习算法二：Naive Bayes [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note17.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note18-1.jpg width="800" />

<a name="laplace-smoothing"><h4>4.4.1. Laplace Smoothing [<sup>目录</sup>](#content)</h4></a> 

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note18-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note19-1.jpg width="800" />

<a name="generalization-for-naive-bayes"><h4>4.4.2. Naive Bayes的一般化 [<sup>目录</sup>](#content)</h4></a>

<a name="more-value-for-x"><h4>4.4.2.1. x<sup>(i)</sup>有多个取值 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note19-2.jpg width="800" />

<a name="multinomial-event-model"><h4>4.4.2.2. 多项式事件模型：考虑词出现的次数 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note19-3.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note20-1.jpg width="800" />

<a name="nolinear-classification"><h2>5. 非线性分类器 [<sup>目录</sup>](#content)</h2></a>

<a name="neural-network"><h3>5.1. Neural Network [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note20-2.jpg width="800" />

<a name="svm"><h3>5.2. SVM [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note20-3.jpg width="800" />

<a name="maximum-margin-classification"><h4>5.2.1. 最大间隔分类器 [<sup>目录</sup>](#content)</h4></a>

<a name="functional-and-geometric-margin"><h4>5.2.1.1. 函数间隔与几何间隔 [<sup>目录</sup>](#content)</h4></a>

<a name="functional-margin"><h4>5.2.1.1.1. 函数间隔 [<sup>目录</sup>](#content)</h4></a> 

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note21.jpg width="800" />

<a name="geometric-margin"><h4>5.2.1.1.2. 几何间隔 [<sup>目录</sup>](#content)</h4></a> 

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note22-1.jpg width="800" />

<a name="maximum-margin-optimization-objective"><h4>5.2.1.2. 最大化间隔分类器优化目标 [<sup>目录</sup>](#content)</h4></a> 

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note22-2.jpg width="400" />

由于约束条件 **||w||=1** 是非凸性约束，这给求解带来困难

<a name="change-optimization-objective"><h4>5.2.1.2.1. 改变优化目标的表述方式 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note23-1.jpg width="800" />

<a name="lagrange-multipliers-approach"><h4>5.2.1.2.2. 拉格朗日乘数法 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note23-2.jpg width="800" />

拉格朗日乘数法的一般化形式

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note24.jpg width="800" />

<a name="dual-problem"><h4>5.2.1.3. 对偶问题 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note25-1.jpg width="800" />

<a name="condtion-primal-problem-equal-to-dual-problem"><h4>5.2.1.3.1 原始问题与对偶问题获得相同解的情况 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note25-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note26-1.jpg width="800" />

<a name="support-vector-machine"><h4>5.2.2. Support Vector Machine [<sup>目录</sup>](#content)</h4></a>

<a name="understand-svm"><h4>5.2.2.1. 对SVM的理解 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-understand-svm-1.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-understand-svm-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-understand-svm-3.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-understand-svm-4.jpg width="800" />

<a name="svm-note-change-and-optimization-objective"><h4>5.2.2.2. 符号改动与优化目标 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note26-2.jpg width="800" />

<a name="svm-dual-problem-and-solving"><h4>5.2.2.3. 对偶问题及求解 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note27-1.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note27-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note28.jpg width="800" />

<a name="svm-inner-product"><h4>5.2.2.4. 算法內积化，引出Kernel [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note29-1.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note29-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note30-1.jpg width="800" />

<a name="kernels"><h4>5.2.3. 核函数Kernels [<sup>目录</sup>](#content)</h4></a>

<a name="define-kernels"><h4>5.2.3.1. 核函数定义 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note30-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note31-1.jpg width="800" />

<a name="kernel-how-to-be-effective"><h4>5.2.3.2. Kernel如何实现高效计算？ [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note31-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note32-1.jpg width="800" />

<a name="validate-kernel"><h4>5.2.3.3. 测试Kernel是否合法 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note32-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note33.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note34-1.jpg width="800" />

<a name="use-kernel-in-svm"><h4>5.2.3.4. Kernel在SVM中的应用 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note34-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note35.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note36-1.jpg width="800" />

<a name="svm-deal-with-unlinear-classification"><h4>5.2.4. 解决非线性决策问题：L<sub>1</sub> Norm Soft Margin SVM [<sup>目录</sup>](#content)</h4></a>

<a name="two-type-unlinear-classification"><h4>5.2.4.1. 两种非线性决策情况 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note36-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note37-1.jpg width="800" />

<a name="soft-margin-svm"><h4>5.2.4.2. L<sub>1</sub> Norm Soft Margin SVM [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note37-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note38-1.jpg width="800" />

<a name="digression-coordinate-ascent"><h4>5.2.5. 坐标上升法 [<sup>目录</sup>](#content)</h4></a>

<a name="original-coordinate-ascent"><h4>5.2.5.1. 原算法 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note38-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note39-1.jpg width="800" />

<a name="smo-algorithm"><h4>5.2.5.2. 算法改进：SMO算法 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note39-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note40.jpg width="800" />

<a name="learning-theory"><h2>6. 学习理论 [<sup>目录</sup>](#content)</h2></a>

<a name="bias-variance-trade-off"><h3>6.1. 偏差-方差权衡 [<sup>目录</sup>](#content)</h3></a>

<a name="understand-bias-and-variance"><h4>6.1.1. 偏差与方差的直观理解 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note50-1.jpg width="800" />

<a name="underfit-and-overfit"><h4>6.1.2. 欠拟合与过拟合 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note41.jpg width="800" />

<a name="empirical-risk-minimization"><h4>6.1.3. ERM（经验风险最小化） [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note42.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note43-1.jpg width="800" />

<a name="prove-erm-bound"><h4>6.1.4. 证明：|ε(h)-ε^(h)| 有上界 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note43-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note44.jpg width="800" />

开始证明：**|ε(h)-ε^(h)| 有上界**

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note45.jpg width="800" />

基于引理（1）得到：

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note46-1.jpg width="800" />

<a name="probability-bound"><h4>6.1.4.1. 概率界形式 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note46-2.jpg width="800" />

<a name="sample-complexity-bound"><h4>6.1.4.2. 样本复杂度界形式 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note47.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note48-1.jpg width="800" />

<a name="error-bound"><h4>6.1.4.3. 误差界形式 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note48-2.jpg width="800" />

误差界 (error bound) 的另一种形式及其推导过程：

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note48-3.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note49-1.jpg width="800" />

<a name="error-bound-benefit-for-quantify-bias-variance-trade-off"><h4>6.1.4.3.1. 有助于量化偏差-方差权衡 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note49-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note50-2.jpg width="800" />

<a name="investigate-sample-compexity"><h4>6.1.5. 探究样本复杂度 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note50-3.jpg width="800" />

<a name="investigate-sample-compexity-on-64bit"><h4>6.1.5.1. 基于浮点数 64bit 的不严谨推论 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note50-4.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note51-1.jpg width="800" />

<a name="investigate-sample-compexity-base-on-vc-dimension"><h4>6.1.5.2. 正式表示形式：基于VC维 [<sup>目录</sup>](#content)</h4></a>

<a name="definition-of-vc-dimension"><h4>6.1.5.2.1. VC维定义 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note51-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note52.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note53-1.jpg width="800" />

<a name="estimate-sample-compexity-base-on-vc-dimension"><h4>6.1.5.2.2. 基于VC维评估样本复杂度 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note53-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note54-1.jpg width="800" />



