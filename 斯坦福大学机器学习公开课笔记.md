<a name="content">目录</a>

[斯坦福大学机器学习公开课笔记](#title)
- [符号约定](#sign)
- [线性回归与优化方法](#linner-regression)
	- [目标函数](#object-function)
	- [优化方法一：批量梯度下降](#batch-gradient-descent)
	- [优化方法二：随机梯度下降](#stochastic-gradient-descent)
	- [优化方法三：线性代数方法](#linear-algebra)


<h1 name="title">斯坦福大学机器学习公开课笔记</h1>

<a name="sign"><h3>符号约定 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note1.jpg width="800" />

<a name="linner-regression"><h3>线性回归与优化方法 [<sup>目录</sup>](#content)</h3></a>

<a name="object-function">目标函数</a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note2-1.jpg width="800" />

<a name="batch-gradient-descent">优化方法一：批量梯度下降 (Batch Gradient Descent)</a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note2-2.jpg width="800" />

<a name="stochastic-gradient-descent">优化方法二：随机梯度下降 (Stochastic Gradient Descent)</a>

随机梯度下降方法与批量梯度下降相比，每次迈出一步不一定是下降最快的方向，有时甚至是相反的方向，但是它的总体趋势是下降的。迭代次数多，但是每次迭代所用的时间少，总体效率更高。

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note3-1.jpg width="800" />

<a name="linear-algebra">优化方法三：线性代数方法</a>

定义矩阵导数，以及矩阵的迹的性质

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note3-2.jpg width="800" />

用线性代数方法求解θ

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note4.jpg width="800" />

最终得到：

<p align="center"> θ = （X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>Y </p>

